# -*- coding: utf-8 -*-
"""Assignment5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZWYLi85uik3iJQ1_s9UYkeK9gYge44d
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
from __future__ import print_function
import tensorflow.keras
import math
import tensorflow as tf
from keras.utils import plot_model
from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation,InputLayer
from tensorflow.keras.layers import AveragePooling2D, Input, Flatten,Dropout,MaxPooling2D,Add,concatenate
from tensorflow.keras.optimizers import Adam,SGD
#from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2
from tensorflow.keras import backend as K
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.models import Model,load_model,Sequential
from sklearn.model_selection import StratifiedShuffleSplit,  train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import random, re, os, cv2
from tensorflow.keras.utils import plot_model
from tensorflow.keras.utils import plot_model
import numpy as np
import pandas as pd
import os
from google.colab.patches import cv2_imshow

!wget http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip
!wget http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip
!unzip Stanford40_ImageSplits.zip
!unzip Stanford40_JPEGImages.zip

!wget https://cdn-31.anonfile.com/x7L7y5m1oa/20a630dd-1586372303/tv_human_interactions_videos.zip
!unzip tv_human_interactions_videos.zip

def lr_schedule(epoch):
    """Learning Rate Schedule

      We got this method from the official keras website but we modified to fit our purpose
      as we will only run for at most 50 epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 1e-3
    if epoch > 30:
        lr *= 1e-3
    elif epoch > 25:
        lr *= 1e-2
    elif epoch > 15:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr




def saveEpochResult(filename, history):
    #We use this function to save our models accuracy and loss

    # convert the history.history dict to a pandas DataFrame:
    hist_df = pd.DataFrame(history.history)

    # save to json:
    with open(filename, mode='w') as f:
        hist_df.to_json(f)

#task 1: preprocess stanford and train it on a model
def preProcessStandford():
  #In this function we preprocess the standford dataset into the require shape and we also split the training set in to train and validation set
  trainSet_f =[]
  testSet_f=[]
  trainSet_=[]
  testSet_=[]
  trainLabel =[]
  testLabel = []
  #read filename
  with open("ImageSplits/train.txt",'r') as f:
    trainSet_f = f.readlines()
  with open("ImageSplits/test.txt",'r') as f:
    testSet_f = f.readlines()
  random.shuffle(trainSet_f)
  random.shuffle(testSet_f)
  #create labels
  for each in trainSet_f:
    trainSet_.append(each.rstrip('\n') )
    label = re.sub('[^A-Za-z]+', '', each)
    trainLabel.append(label)
  for each in testSet_f:
    testSet_.append(each.rstrip('\n') )
    label = re.sub('[^A-Za-z]+', '', each)
    testLabel.append(label)

  #read and resize training and test images
  trainSet = []
  path = "JPEGImages/"
  dim =(224,224)
  for imgs in trainSet_:
    fname = path + imgs
    im = cv2.imread(fname)
    try:
      img = cv2.resize(im, dim)
    except Exception as e:
      print(str(e))
    trainSet.append(img)

  testSet = []
  path = "JPEGImages/"
  dim =(224,224)
  for imgs in testSet_:
    fname = path + imgs
    im = cv2.imread(fname)
    try:
      img = cv2.resize(im, dim)
    except Exception as e:
      print(str(e))
    testSet.append(img)

  #convert labels to numerical
  encoder = LabelEncoder()
  encoder.fit(trainLabel)
  encoded_Y = encoder.transform(trainLabel)
  onehotencoder = OneHotEncoder()
  train_label = onehotencoder.fit_transform(encoded_Y.reshape(-1,1)).toarray()
  #train_label = np.asarray(encoded_Y)

  encoder.fit(testLabel)
  encoded_Y_ = encoder.transform(testLabel)
  test_label = onehotencoder.fit_transform(encoded_Y_.reshape(-1,1)).toarray()

  #get validation set from training data
  X_train, X_test, y_train, y_test = train_test_split(trainSet, train_label,
                                                      test_size=0.1,random_state=20, stratify =train_label )

  #normalize training, validation and test data
  X_train = np.asarray(X_train)
  X_test = np.asarray(X_test)
  testSet = np.asarray(testSet)




  X_train = X_train.reshape((X_train.shape[0], 224, 224, 3))
  X_test = X_test.reshape((X_test.shape[0], 224, 224, 3))
  testSet = testSet.reshape((testSet.shape[0], 224, 224, 3))

  X_train = X_train.astype('float32')
  X_test = X_test.astype('float32')
  testSet = testSet.astype('float32')

  X_train = X_train/255.0
  X_test =  X_test/255.0
  testSet = testSet/255.0

  return X_train, X_test, y_train, y_test, testSet, test_label

def identity_block(X, k, filters, stage, block, flowname):
  #This function defines the resnet version 1 structure, we have both a residual and conv block to
  # account for skip layer and changes in input dimmension
    conv_name_base = 'res' + str(stage) +"_"+ block + '_branch'+flowname
    bn_name_base = 'bn' + str(stage)  +"_"+block + '_branch'+flowname

    # Retrieve Filters
    F1, F2, F3 = filters

    # Save the input value. We'll need this later to add back to the main path. 
    X_shortcut = X

    # First component of main path: commented first layer was used to test the kernel regularizer choice task
    #X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = "he_normal",kernel_regularizer=l2(1e-4))(X)
    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = "he_normal",kernel_regularizer=l2(1e-4))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)
    X = Activation('relu')(X)
    X= Dropout(0.05)(X)

    # Second component of main path
    #X = Conv2D(filters = F2, kernel_size = (k, k), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = "he_normal",kernel_regularizer=l2(1e-4))
    X = Conv2D(filters = F2, kernel_size = (k, k), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = "he_normal",kernel_regularizer=l2(1e-4))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)
    X = Activation('relu')(X)
    X= Dropout(0.05)(X)

    # Third component of main path
    #X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c'+flowname,kernel_initializer = "he_normal", kernel_regularizer=l2(1e-4))(X)
    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c'+flowname,kernel_initializer = "he_normal")(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)
    #conv-block
    #X_shortcut = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '1', kernel_initializer = "he_normal",kernel_regularizer=l2(1e-4))(X_shortcut)
    X_shortcut = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '1', kernel_initializer = "he_normal")(X_shortcut)
    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)

    # Final step: Add shortcut value to main path, and pass it through a RELU activation
    X = Add()([X, X_shortcut])
    X = Activation('relu')(X)
    X= Dropout(0.05)(X)
    return X

def RESNet18_stanford(input_dim, input_name):
  #The method define a resnet 18 model, we call this model architecture through out the code.
  #It uses identityBlock method that defines the structure conv_batch_normalization-activation. 
  #we use it for just standford cause of how we name the layers
  x_input = Input(input_dim)
      # Stage 1
  #X = Conv2D(64, (7, 7), strides = (2, 2), name = input_name, kernel_initializer = 'he_normal',kernel_regularizer=l2(1e-4))(x_input)
  X = Conv2D(64, (7, 7), strides = (2, 2), name = input_name, kernel_initializer = 'he_normal')(x_input)
  X = BatchNormalization(axis = 3, name = 'bn_conv_input_stan')(X)
  X = Activation('relu')(X)
  X = MaxPooling2D((3, 3), strides=(2, 2))(X)
  #[64, 128, 256]
  X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv2_s', kernel_initializer = 'he_normal')(X)
  X = identity_block(X, 3, [64, 64, 256], stage=2, block='b', flowname="stan")
  X = identity_block(X, 3, [64, 64, 256], stage=2, block='c', flowname="stan")

  #X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv3_s', kernel_initializer = 'he_normal',kernel_regularizer=l2(1e-4))(X)
  X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv3_s', kernel_initializer = 'he_normal')(X)
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='b', flowname="stan")
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='c', flowname="stan")
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='d', flowname="stan")

      # AVGPOOL.
  X = AveragePooling2D((2, 2), name='avg_pool_s')(X)

    # output layer
  X = Flatten()(X)

  return X, x_input

def RESNet18(input_dim, classes):
  #Here we train on the standford dataset using our predefined res18 architecture
  x_input = Input(input_dim)
  X, x_input= RESNet18_stanford(input_dim,"conv_stanford")

  X = Dense(classes, activation='softmax', name='fc_layer_s' + str(classes), kernel_initializer = 'he_normal')(X)
    # Create model

  model = Model(inputs = x_input, outputs = X, name='ResNet18_conv_stanford')

  model.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])
  X_train, X_test, y_train, y_test, testSet, test_label = preProcessStandford()
  datagen = ImageDataGenerator(zca_epsilon=1e-06,width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True)
  datagen.fit(X_train)
  lr_scheduler = LearningRateScheduler(lr_schedule)

  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=5, min_lr=1e-5, verbose=1)
  callbacks = [lr_scheduler,lr_reducer]
  history = model.fit_generator(datagen.flow(X_train, y_train,batch_size=64),validation_data=(X_test, y_test),epochs=1,
                                verbose=2,callbacks = callbacks,workers=1)
  saveEpochResult("rs18_standford_accuracy.json", history)
  loss,acc = model.evaluate(testSet, test_label, verbose=2)
  print("Validation accuracy and loss: ", acc,loss)
  
  model.save_weights("res18_standford.h5")
  model.load_weights("res18_standford.h5",by_name=True)

  return model

model_standford.summary()

#task2: transferlearning for videodataset: preprocess, load model,transfer

def preProcessVideos(hand_shake_test, high_five_test, hug_test, kiss_tets):
  #In this function we preprocess the video dataset and then we call it inside the function,
  # videoDataset to extract the training and testset respectively
  test_set = []
  for i in hand_shake_test:
    if (i < 10):
      filename = "handShake_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "handShake_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in high_five_test:
    if (i < 10):
      filename = "highFive_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "highFive_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in hug_test:
    if (i < 10):
      filename = "hug_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "hug_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in kiss_tets:
    if (i < 10):
      filename = "kiss_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "kiss_00" + str(i) + ".avi"
      test_set.append(filename)
  random.shuffle(test_set)
  test_label =[]
  #get labels
  for each in test_set:
    label = re.sub('[^A-Za-z]+', '', each)
    test_label.append(label)
  #convert labels to numerical
  encoder = LabelEncoder()
  encoder.fit(test_label)
  encoded_Y = encoder.transform(test_label)
  onehotencoder = OneHotEncoder()
  test_label_ = onehotencoder.fit_transform(encoded_Y.reshape(-1,1)).toarray()
  #read frame, resize and convert
  test_set_ = []
  dim =(224,224)
  for each in test_set:
    fname = "tv_human_interactions_videos/"+ each
    cap = cv2.VideoCapture(fname)
    middleframe = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) *0.5)
    cap.set(1,middleframe); # Where frame_no is the frame you want
    ret, frame = cap.read() # Read the frame
    try:
      img = cv2.resize(frame, dim)
    except Exception as e:
      print(str(e))
    test_set_.append(img)
  testSet__ = np.asarray(test_set_)
  testSet__ = testSet__.reshape((testSet__.shape[0], 224, 224, 3))
  testSet__ = testSet__.astype('float32')
  testSet__ = testSet__ / 255.0
  return testSet__, test_label_

def videoDataset():
  #In this function, we use te preProcessVideos function to load and process the images
  
  hand_shake_test = [2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50]
  high_five_test = [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48]
  hug_test = [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50]
  kiss_tets = [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]
  hand_shake_train = [1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39]
  high_five_train = [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50]
  hug_train = [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48]
  kiss_train = [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]
  testSet , testLabel = preProcessVideos(hand_shake_test, high_five_test, hug_test, kiss_tets)
  trainSet , trainLabel = preProcessVideos(hand_shake_train, high_five_train, hug_train, kiss_train)
  #get validation set from training data
  X_train, X_test, Y_train, Y_test = train_test_split(trainSet, trainLabel,
                                                      test_size=0.1, stratify=trainLabel,random_state=2)
  return X_train, X_test, Y_train, Y_test, testSet , testLabel

def transferLearnVideos():
  #In this function, we use the weights from the stanford model and fit it to a new model with the same architecture but different output layer
  #we only re train the last 2 conv layers
  model_frames = RESNet18((224,224,3), 40)

  model_ = Model(model_frames.input,model_frames.layers[-2].output)
  lastlayer = model_frames.layers[-2].output
  lastlayer_ = Dense(4, activation='softmax', name='fc_layer_tr' + str(4), kernel_initializer = 'he_normal')(lastlayer) 
  trsf_model = Model(model_.input, lastlayer_)

  X_train_v, X_test_v, Y_train_v, Y_test_v, testSet_v , testLabel_v = videoDataset()
  trsf_model.trainable = True
  set_trainable = False
  for layer in trsf_model.layers:
    if layer.name in ['res3_d_branchstan2cstan', 'res3_d_branchstan1', 'actactivation_15']: #we unfreeze last 3 layers
      set_trainable = True
    if set_trainable:
      layer.trainable = True
    else:
      layer.trainable = False

  
  trsf_model.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])
  history = trsf_model.fit(X_train_v, Y_train_v, epochs= 10, validation_data=(X_test_v, Y_test_v), verbose=2,batch_size=64)
  loss,acc = trsf_model.evaluate(testSet_v, testLabel_v, verbose=2)
  print("Validation accuracy and loss(frame): ", acc,loss)
  saveEpochResult("rs18_video_accuracy.json", history)
  plot_model(trsf_model ,to_file='video_frame_model.png',show_shapes=True)
  trsf_model.save_weights("res18_videos.h5")
  trsf_model.load_weights("res18_videos.h5",by_name=True)

  return trsf_model

model_frame = transferLearnVideos()

#task3: ccalculate optical frame

def RESNet18_videos(input_dim, input_name):
  #The method define a resnet 18 model, we call this model architecture through out the code.
  #It uses identityBlock method that defines the structure conv_batch_normalization-activation. 
  #we use it for just opticalframe dataset cause of how we name the layers
  x_input = Input(input_dim)
      # Stage 1
  X = Conv2D(64, (7, 7), strides = (2, 2), name = input_name, kernel_initializer = 'he_normal')(x_input)
  X = BatchNormalization(axis = 3, name = 'bn_conv_input_vid')(X)
  X = Activation('relu')(X)
  X = MaxPooling2D((3, 3), strides=(2, 2))(X)
  #[64, 128, 256]
  X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv2_vid', kernel_initializer = 'he_normal')(X)
  X = identity_block(X, 3, [64, 64, 256], stage=2, block='b', flowname="video")
  X = identity_block(X, 3, [64, 64, 256], stage=2, block='c', flowname="video")


  X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv3_vid', kernel_initializer = 'he_normal')(X)
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='b', flowname="video")
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='c', flowname="video")
  X = identity_block(X, 3, [128, 128, 512], stage=3, block='d', flowname="video")

      # AVGPOOL.
  X = AveragePooling2D((2, 2), name='avg_pool_v')(X)

    # output layer
  X = Flatten()(X)

  return X, x_input

def stackOpticalFrame():
  #In this function, we  calculate the optical flow and stack them into the shape (224,224,16)
  
  hand_shake_test = [2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50]
  high_five_test = [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48]
  hug_test = [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50]
  kiss_tets = [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]

  hand_shake_train = [1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39]
  high_five_train = [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50]
  hug_train = [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48]
  kiss_train = [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50] 
  

  
  test_set = []
  for i in hand_shake_test:
    if (i < 10):
      filename = "handShake_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "handShake_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in high_five_test:
    if (i < 10):
      filename = "highFive_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "highFive_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in hug_test:
    if (i < 10):
      filename = "hug_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "hug_00" + str(i) + ".avi"
      test_set.append(filename)
  for i in kiss_tets:
    if (i < 10):
      filename = "kiss_000" + str(i) + ".avi"
      test_set.append(filename)
    if (i >=10):
      filename = "kiss_00" + str(i) + ".avi"
      test_set.append(filename)
  random.shuffle(test_set)
  test_label =[]
  #get labels
  for each in test_set:
    label = re.sub('[^A-Za-z]+', '', each)
    test_label.append(label)
  #convert labels to numerical
  encoder = LabelEncoder()
  encoder.fit(test_label) 
  encoded_Y = encoder.transform(test_label)
  all_arrays = []
  test_Label =[]
  test_label__ = []
  counter = 0
  value  = 0 
  encodeY_len = len(encoded_Y) 
 
 #----adjusting the labels so they match the optical flow-----
  while (value < encodeY_len ):

    value = math.floor(counter/16) 
    test_label__.append(encoded_Y[value]) 
    counter += 1
    
    if (counter %16 ==0):

      test_Label.append(np.unique(test_label__))
      test_label__ = []

    if(counter == 1600):
      break

  test_Label = np.asarray(test_Label)
  onehotencoder = OneHotEncoder() 
  test_Label = onehotencoder.fit_transform(test_Label).toarray() 
#------------
#read video, calculate optical flow, convert to greyscale and stack
  for each in test_set:
    dim =(224,224)
    fname ="tv_human_interactions_videos/"+ each
    cap = cv2.VideoCapture(fname)
    ret, frame1 = cap.read()
    test_set_ = []
    prsv = cv2.resize(frame1, dim)
    prsv_ = cv2.cvtColor(prsv,cv2.COLOR_BGR2GRAY)
    #print("prsv",prsv_.size)
    hsv = np.zeros_like(prsv)
    hsv[...,1] = 255
    totalframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    framejump = totalframes/16
    i = 0
    while(i < 16):
      cap.set(1,framejump*i)
      ret, frame2 = cap.read()
      try:
        img = cv2.resize(frame2, dim)
      except Exception as e:
        print(str(e))
      
      next = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
      #print("img",img.size)
      flow = cv2.calcOpticalFlowFarneback(prsv_,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
      mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
      hsv[...,0] = ang*180/np.pi/2
      hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
      rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
      rgb_ = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)
      test_set_.append(rgb_)
      i = i+1 
    all_arrays.append(test_set_)
  all_arrays = np.asarray(all_arrays)
  all_arrays=np.moveaxis(all_arrays, 1, -1)


  #split into test nd valid?: convert shape and divide by 255
  all_arrays = all_arrays.reshape((all_arrays.shape[0], 224, 224, 16))
  all_arrays = all_arrays.astype('float32')
  all_arrays = all_arrays / 255.0

  X_train, X_test, Y_train, Y_test = train_test_split(all_arrays, test_Label, test_size=0.1, stratify=test_Label,random_state=2)
  
  return X_train, X_test, Y_train, Y_test


def trainOpticalFrame():
  #In this function, we train the calculated optical frames using transfer learning to set some of the weights
  model_frames = transferLearnVideos()
  X, inp = RESNet18_videos((224,224,16),"conv_input_optical")

  X = Dense(4, activation='softmax', name='fc_layer_op' + str(4), kernel_initializer = 'he_normal')(X)
    # Create model

  opticalflow = Model(inputs = inp, outputs = X, name='ResNet18_opticalflow')

  #pretrained_model = transferLearnVideos()

  print(len(opticalflow.layers))
  for i in range(5, len(opticalflow.layers)):
    if opticalflow.layers[i].name not in ['res3_d_branchstan2cstan', 'res3_d_branchstan1', 'actactivation_15']:
      opticalflow.layers[i].set_weights(model_frames.layers[i].get_weights())
      #opticalflow.layers[i].trainable = False
    # else:
    #   opticalflow.layers[i].trainable = True #we retrain the first layers and the last layers


  X_train, X_test, Y_train, Y_test = stackOpticalFrame()
  opticalflow.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])
  history = opticalflow.fit(X_train, Y_train, epochs= 10, validation_data=(X_test, Y_test), verbose=2,batch_size=64)
  plot_model(opticalflow ,to_file='video_opticalfow_model.png',show_shapes=True)
  saveEpochResult("rs18_opflow_accuracy.json", history)
  opticalflow.save_weights("res18_opflow.h5")
  opticalflow.load_weights("res18_opflow.h5",by_name=True)

  return opticalflow

model_optflow = trainOpticalFrame()

#task 4: create two stream model:
def twoStreamCNN_concatenate():
  #In this veersion we use minimum in creating the fusion layer for the two stream
  X_train_op, X_test_op, Y_train_op, Y_test_op = stackOpticalFrame()
  X_train_frame, X_test_frame, Y_train_f, Y_test_f, testSet , testLabel = videoDataset()
  
  model_flow = trainOpticalFrame()
  model_frame = transferLearnVideos()
  model_frame.trainable=False
  model_flow.trainable=False
  image_x = model_frame.layers[-1].output
  flow_x = model_flow.layers[-1].output

  x =tf.keras.layers.concatenate(axis=1)([model_frame.layers[-33].output, model_flow.layers[-33].output])
  x = Dropout(0.1)(x)
  x = Conv2D(512, (3, 3),  name = 'extra_conv_fusion', kernel_initializer = 'he_normal', activation='relu',kernel_regularizer=l2(1e-4) )(x)
  x = Conv2D(512, (3, 3), name = 'extra_conv_fusion_2', kernel_initializer = 'he_normal',activation='relu')(x)
  x = Flatten()(x)
  #x = Add(name= "combined_output")([image_x,flow_x])
  outp = Dense(4, name="new_output_fusion")(x)
  model_fusion = Model([model_frame.input, model_flow.input], outp)
  #img = plot_model(model_fusion, to_file='model.png')

  model_fusion.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])
  history = model_fusion.fit([X_train_frame,X_train_op],Y_train_op, 
                             validation_data=([X_test_frame, X_test_op],Y_test_op ), 
                             epochs=30, batch_size= 64, verbose=2)
  plot_model(model_fusion ,to_file='video_twostreams_model.png',show_shapes=True)
  saveEpochResult("rs18_twostrammodel_accuracy.json", history)
  model_fusion.save_weights("twostreamCNN.h5")
  model_fusion.load_weights("twostreamCNN.h5")
  #model.fit()
  return model_fusion
def twoStreamCNN_minimum():
  #In this veersion we use minimum in creating the fusion layer for the two stream
  X_train_op, X_test_op, Y_train_op, Y_test_op = stackOpticalFrame()
  X_train_frame, X_test_frame, Y_train_f, Y_test_f, testSet , testLabel = videoDataset()
  
  model_flow = trainOpticalFrame()
  model_frame = transferLearnVideos()
  model_frame.trainable=False
  model_flow.trainable=False
  image_x = model_frame.layers[-1].output
  flow_x = model_flow.layers[-1].output

  x =tf.keras.layers.minimum(([model_frame.layers[-33].output, model_flow.layers[-33].output]))
  x = Dropout(0.1)(x)
  x = Conv2D(512, (3, 3),  name = 'extra_conv_fusion', kernel_initializer = 'he_normal', activation='relu',kernel_regularizer=l2(1e-4) )(x)
  x = Conv2D(512, (3, 3), name = 'extra_conv_fusion_2', kernel_initializer = 'he_normal',activation='relu')(x)
  x = Flatten()(x)
  #x = Add(name= "combined_output")([image_x,flow_x])
  outp = Dense(4, name="new_output_fusion")(x)
  model_fusion = Model([model_frame.input, model_flow.input], outp)
  #img = plot_model(model_fusion, to_file='model.png')

  model_fusion.compile(loss='categorical_crossentropy',optimizer="adam",metrics=['accuracy'])
  history = model_fusion.fit([X_train_frame,X_train_op],Y_train_op, 
                             validation_data=([X_test_frame, X_test_op],Y_test_op ), 
                             epochs=30, batch_size= 64, verbose=2)
  plot_model(model_fusion ,to_file='video_twostreams_model.png',show_shapes=True)
  saveEpochResult("rs18_twostrammodel_accuracy.json", history)
  model_fusion.save_weights("twostreamCNN.h5")
  model_fusion.load_weights("twostreamCNN.h5")
  #model.fit()
  return model_fusion

model_stream =twoStreamCNN_minimum()

